{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM and Prompting Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Call and Components\n",
    "\n",
    "1) LLM Access Authorization setup\n",
    "2) Prompt Template\n",
    "3) Messages:\n",
    "    - System Prompt\n",
    "    - User Prompt\n",
    "    - Assistant Prompt (AI prompt)\n",
    "4) LLM Function call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UseCase: Social Media Post Generator\n",
    "### 1) LLM Access Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM API Access setup\n",
    "import openai\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Prompt Template\n",
    "- Template to collate static and dynamic prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "product_name = \"EcoLight Smart Bulb\"\n",
    "cool_feature = \"Energy-saving with customizable colors and voice control\"\n",
    "audience_persona = \"Environmentally conscious homeowners\"\n",
    "\n",
    "# Social Media Marketing Assistant\n",
    "prompt_template = f\"\"\"Create a catchy, clever 140 character social media post targeted toward {audience_persona} introducing and promoting a new {cool_feature} feature of {product_name}.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Messages\n",
    "    - System Prompt : Set global AI behavior\n",
    "    - User Prompt : Prompts from the user\n",
    "    - Assistant Prompt (AI prompt) : Responses from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create a catchy, clever 140 character social media post targeted toward Environmentally conscious homeowners introducing and promoting a new Energy-saving with customizable colors and voice control feature of EcoLight Smart Bulb.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"You are a social media influencer and writer.\"\n",
    "user_prompt = prompt_template.format(audience_persona, cool_feature, product_name)\n",
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "          {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "          }\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) LLM Function call\n",
    "\n",
    "**Function Call Arguments**\n",
    "- temperature:\tControls randomness: higher = more creative, lower = more focused.\n",
    "- top_p: \tNucleus sampling: limits token choices to top probability mass p.\n",
    "- max_tokens:\tLimits length of the generated response (in tokens).\n",
    "- frequency_penalty:\tPenalizes repeated tokens to reduce verbosity/repetition.\n",
    "- presence_penalty:\tEncourages introducing new topics by discouraging reuse of existing ones.\n",
    "- messages:\tProvides chat history as a list of user/assistant/system turns.\n",
    "- model:\tChooses which LLM to use (e.g., gpt-4, gpt-3.5-turbo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Social Media Post:\n",
      "\"ðŸŒ¿ðŸ’¡ Illuminate your home eco-consciously with the new EcoLight Smart Bulb! Save energy, set customizable colors, all with voice control. Your light, your way! #EcoFriendly #SmartHome\"\n"
     ]
    }
   ],
   "source": [
    "# Function to call the OpenAI GPT-3.5 API\n",
    "def get_response(prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1\n",
    "        )\n",
    "        # The response is a JSON object containing more information than the generated post. We want to return only the message content\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Generating the social media post\n",
    "generated_post = get_response(prompt_template)\n",
    "\n",
    "# Printing the output. \n",
    "print(\"Generated Social Media Post:\")\n",
    "print(generated_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Best Practices\n",
    "\n",
    "1) Be Specific and Clear\n",
    "2) Use Delimiters to set focus\n",
    "3) Explicitly state output requirements and format\n",
    "4) For Complex tasks, request step by step\n",
    "5) Role Playing method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic completion function\n",
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Be Specific and Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, couldn't find a movie to recommend today.\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommend movies to a customer.\n",
    "You are responsible to recommend a movie from a list of top global trending movies like {global_trending_movies}.\n",
    "If the customer specifies a type of movie then pick the movie from the list that closely matches customer request. \n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"Please recommend a movie based on my interests.\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specific request as below are better than generic response like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I recommend you watch \"The Suicide Squad\" as it is a popular superhero movie from the list of top global trending movies. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommends movies to a customer.\n",
    "You are responsible to recommend a movie from a list of top global trending movies from {global_trending_movies}.\n",
    "If the customer specifies a type of movie then pick the movie from the list that closely matches customer request. \n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"I love super-hero movies. Please recommend a movie based on my interests.\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Add Delimiters\n",
    "\n",
    "- delimiters help to focus LLM to a specific part of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python```python\n",
       "strings2.append(\"one\")\n",
       "strings2.append(\"two\")\n",
       "strings2.append(\"THREE\")\n",
       "strings2.append(\"4\")\n",
       "```\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "prompt = \"\"\"\n",
    "Convert the following code block in the #### <code> #### section to Python:\n",
    "\n",
    "####\n",
    "strings2.push(\"one\")\n",
    "strings2.push(\"two\")\n",
    "strings2.push(\"THREE\")\n",
    "strings2.push(\"4\")\n",
    "####\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Explicitly state output requirements and format\n",
    "\n",
    "LLM are stochastic in the output format unless it has prompts that specify the output hence explicitly state how you want your output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"product_name\": \"Nike Air Max 270 React\",\n",
      "    \"product_brand\": \"Nike\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
    "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
    "\n",
    "###\n",
    "product_name: the name of the product\n",
    "product_bran: the name of the brand (if any)\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) For Complex tasks, request step by step\n",
    "\n",
    "To elicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odd numbers in the group are 15, 5, 13, 7, and 1.\n",
      "\n",
      "Adding these odd numbers together: 15 + 5 + 13 + 7 + 1 = 41.\n",
      "\n",
      "Since 41 is an odd number, the sum of the odd numbers in the group is odd.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response= get_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Role Playing method\n",
    "\n",
    "Role Playing is another technique to elicit a specific behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are formed when massive stars exhaust their nuclear fuel and undergo a supernova explosion. If the core of the star is massive enough, it collapses under its own gravity, forming a singularity - a point of infinite density. This singularity is surrounded by an event horizon, beyond which nothing, not even light, can escape. This creates a region of spacetime with extremely strong gravitational effects, known as a black hole.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
    "\"\"\"\n",
    "\n",
    "user_message_1 = \"\"\"\n",
    "Hello, who are you?\n",
    "\"\"\"\n",
    "\n",
    "ai_message_1 = \"\"\"\n",
    "Greeting! I am an AI research assistant. How can I help you today?\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Human: Can you tell me about the creation of blackholes?\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_1\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ai_message_1\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
